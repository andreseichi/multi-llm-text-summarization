1 Introduction
Long documents, such as scientific papers and government reports, often discuss substantial issues at
length, and thus are time-consuming to read, let
alone to comprehend. Generating abstractive summaries can help readers quickly grasp the main
topics, yet prior work has mostly focused on short
texts (containing hundreds of words), e.g., news
articles (Gehrmann et al., 2018; Liu and Lapata,
2019; Zhang et al., 2019).
Model training efficiency and summary quality
present a pair of challenges for long document
summarization. State-of-the-art systems (Lewis
et al., 2020; Zhang et al., 2019) are built upon
Transformer (Vaswani et al., 2017), which uses attentions to compute pairwise relations between tokens. Such framework has quadratic time and memory complexities, and is too costly for long documents 1
. Solutions have been proposed to reduce
1
For instance, to fine-tune BART on documents of 10K
the calculation of encoder self-attentions (Wang
et al., 2020c; Zaheer et al., 2020) by selectively attending to neighboring tokens (Beltagy et al., 2020;
Child et al., 2019) or relevant words (Kitaev et al.,
2020; Tay et al., 2020a). Yet, these methods do not
apply to encoder-decoder attentions in summarization models since they collaborate and dynamically
pinpoint salient content in the source as the summary is decoded. Truncation is commonly used
to circumvent the issue. However, training on curtailed content further aggravates “hallucination” in
existing abstractive models (Maynez et al., 2020).
We argue that summarizing long documents
(e.g., with thousands of words or more) requires efficient handling of both types of attentions. To this
end, we propose an efficient encoder-decoder attention with head-wise positional strides (HEPOS),
where the attention heads follow a strided pattern
and have varying starting positions. HEPOS reduces computational and memory costs while (1)
maintaining the power of emphasizing important
tokens, and (2) preserving the global context per
head. HEPOS successfully doubles the processed
input sequence size, when combined with any encoder. To the best of our knowledge, we are the
first to study efficient encoder-decoder attentions
and provide a systematic comparison of diverse
encoder attentions for the task of summarization.2
For evaluation, we collect a new large-scale
dataset, GOVREPORT, consisting of about 19.5k
U.S. government reports with expert-written abstractive summaries.3 GOVREPORT has two important features: (1) It contains significantly longer
documents (9.4k words) and summaries (553
words) than existing datasets, such as PubMed and
arXiv (Cohan et al., 2018) (see Table 2); (2) Salient
tokens with a batch size of 1, 70GB of memory is needed for
encoder attentions, and 8GB for encoder-decoder attentions.
2Our code is released at https://github.com/
luyang-huang96/LongDocSum.
3GOVREPORT can be downloaded from https://
gov-report-data.github.io.
arXiv:2104.02112v2 [cs.CL] 11 Apr 2021
content is spread throughout the documents, as opposed to cases where summary-worthy words are
more heavily concentrated in specific parts of the
document. These properties make GOVREPORT an
important benchmark for producing long document
summaries with multiple paragraphs.
We conduct experiments on GOVREPORT and
scientific papers in PubMed and arXiv. First,
when summarizing documents of the same length,
HEPOS attention yields significantly better ROUGE
scores than a non-trivial comparison that projects
attentions into low-rank space (Wang et al., 2020c).
Second, when trained on the same GPU, HEPOS
attention, combined with sparse encoder attentions,
is able to read more than 10K words and obtains significantly higher ROUGE scores on GOVREPORT
and new state-of-the-art results on PubMed, compared with full encoder-decoder attention models
which can process at most 5K input words. Human
judges further rate the summaries generated by our
models to be more informative and faithful.
We further propose a new evaluation metric
for faithfulness, inspired by APES (Eyal et al.,
2019), a fill-in-the-blank QA metric for summary
evaluation. With questions generated from references, our metric, APESsrc, compares QA answers
by reading the source and the system summary. It is
shown to be better correlated with human judgment
than the original metric and an entailment-based
scorer (Kryscinski et al., 2020).
The rest of the paper is organized as follows. We
describe efficient encoder attentions in prior work
in § 2, and formulate our proposed encoder-decoder
attention in § 3. The GOVREPORT data is presented
in § 4. We then share details on evaluation metrics
(§ 5) and experimental results (§ 6). Additional
related work is listed in § 7, with conclusion in §8.
2 Prior Work on Efficient Encoder
Attentions
Transformer models are built upon multi-head attentions in multiple layers. The attention is calculated as Attention(Q, K, V) = softmax(
QKT
√
dk
)V,
where Q, K, and V are query, key, and value matrices, each consisting of n vectors for a document
with n tokens, thus the quadratic memory footprint.
Here, we present an overview of representative methods for efficient encoder self-attentions
(henceforth “encoder attentions”) that can be
built upon large pre-trained seq2seq models, e.g.,
BART (Lewis et al., 2020). We follow the naming
Model Complexity # New Para.
Full O(n
2
) —
Encoder Self-attentions
I. Fixed Patterns
Sliding Window (2020) O(nw) 0
Adaptive Span (2019) O(nwˆ) O(1)
Global Tokens (2020) O(2ng) 0
Stride (2019) O(n
2
/s) 0
Random (2020) O(nr) 0
II. Low-rank
Linformer (2020c) O(nk) O(n)
III. Learnable Patterns
LSH (2020) O(lnbl) 0
Sinkhorn (2020a) O(2nbs) 0
Encoder-decoder Attentions
Hepos (ours) O(mn/sh) 0
Linformer O(mk) O(n)
Table 1: Summary of efficient Transformer attentions
on memory complexity and newly learned parameters
compared with full attentions at each layer. m and n
are lengths of the input and the output. See § 2 and § 3
for model-specific hyperparameters.
convention of Tay et al. (2020b), and summarize
their memory complexities and numbers of newly
learned parameters in Table 1.
2.1 Fixed Patterns
Fixed patterns are used to limit the scope of attentions. In our experiments, in addition to windowbased attentions, we also combine them with global
tokens, stride patterns, or random attentions.
Sliding window attentions (Beltagy et al., 2020)
aim to capture the local context, which is critical for
language understanding (Liu* et al., 2018; Child
et al., 2019). Concretely, each query token attends
to w/2 neighboring tokens on both left and right,
yielding a memory complexity of O(nw).
Adaptive span is proposed by Sukhbaatar et al.
(2019) to learn attention windows at different layers. This is implemented by learning a masking
function for each head independently. In practice,
the adaptive span attention has a complexity of
O(nwˆ), where wˆ is the maximum values of predicted spans for all heads. Besides, it introduces
O(1) new parameters for learning spans.
Global tokens (Beltagy et al., 2020) are often
added to sliding windows to let pre-selected tokens
attend to the full sequence, to build global representations. Importantly, global attention operations are
symmetric, i.e., a global token is also attendable
to all tokens in the sequence. We select the first g
tokens as global tokens, as leading sentences are
often important for summarization. Memory complexity is O(2ng) due to the symmetric attentions.
Stride patterns are proposed by Child et al. (2019)
to capture long term interactions, where each query
attends to every s-th token, with s as the stride size.
It thus has a complexity of O(n
2/s).
Random attention is motivated by the fact that
randomly constructed graphs with Θ( ˜ n) edges can
approximate the complete graphs spectrally (Zaheer et al., 2020). Zaheer et al. (2020) propose
to allow each query to attend to r random keys,
resulting in a complexity of O(nr). For efficient
implementations, input tokens are first segmented
into blocks. Tokens in the same block attend to
tokens in another randomly selected block.
2.2 Low-rank Methods
Wang et al. (2020c) show that self-attention matrices are low-rank. They propose Linformer that
linearly projects key and value matrices into a lowdimensional space, e.g., from n to k, to achieve a
O(nk) complexity. It also introduces O(n) new
parameters for projection matrix learning.
2.3 Learnable Patterns
Recently, learnable sparse attentions are proposed
to better capture both local and global contexts than
attentions based on fixed patterns.
Locality-sensitive hashing (LSH) attentions use
a random-projection hashing function to hash similar queries and keys into the same buckets in l
rounds (Kitaev et al., 2020). Attentions are then
computed among tokens within each bucket. For
bucket size bl
, the complexity of LSH attention is
O(lnbl).
Sinkhorn attentions first segment a sequence into
blocks, which are then arranged by a learned
Sinkhorn sorting network (Tay et al., 2020a). Given
the new permutation, each query attends to bs tokens within the same block to maintain the local
context and another bs tokens in a neighboring
block to capture global interactions. Its complexity
is O(2nbs).
2.4 Other Attentions
We also describe several notable methods that are
not suitable for our experiments and excluded from
this study: Recurrence over input segments are
tailored for an autoregressive decoder only (Dai
et al., 2019); memory methods use a separate memory module to attend to full sequences (Lee et al.,
head1head2head3head4Encoder KeyHeposAttentionGAOwasasked... homecareDecoderQueryJobin...
...
Figure 1: A toy example of our HEPOS attention, with
a stride of 2 and four attention heads. Dark colors indicate that heads 1 and 3 attend to the first and third
tokens (“Job" and “home") in the input, heads 2 and 4
look at the second and fourth words (“in" and “care").
2019), which share a similar theoretical foundation
as global tokens; and kernel methods over attentions require training models from scratch (Choromanski et al., 2020; Katharopoulos et al., 2020).
3 Encoder-decoder Attention with
Head-wise Positional Strides (Hepos)
The efficient design of encoder-decoder attentions
with head-wise positional strides (HEPOS) allows
models to consume longer sequences. Concretely,
our design is motivated by two observations: (1)
Attention heads are redundant (Voita et al., 2019).
(2) Any individual head rarely attends to several
tokens in a row (Clark et al., 2019). Therefore, as
illustrated in Fig. 1, HEPOS uses separate encoderdecoder heads on the same layer to cover different
subsets of source tokens at fixed intervals. Each
head starts at a different position, and all heads
collectively attend to the full sequence.
Given a stride size of sh, for the h-th head, its
attention value between decoder query qj (at step
j) and encoder key vector ki (for the i-th input
token) can be formulated as:
a
h
ji =
(
softmax(qjki), if (i − h) mod sh = 0
0 otherwise
(1)
In HEPOS attention, each query token attends to
n/sh tokens per head, yielding a memory complexity of O(mn/sh), where m is the output length.
For comparison, Linformer (§ 2.2) can be
straightforwardly adapted for encoder-decoder attentions by using decoder queries for attention calculation instead. We do not adapt pattern-based
attentions (§ 2.1 and § 2.3), since they rely on local
token grouping which makes it difficult to pinpoint
salient content.
4 GOVREPORT Dataset
We introduce a new large-scale dataset, GOVREPORT, containing 19, 466 long reports published by
U.S. Government Accountability Office (GAO)4
to fulfill requests by congressional members, and
Congressional Research Service (CRS)5
, covering
researches on a broad range of national policy issues. A human-written summary is provided along
with each report. During data collection, we remove boilerplates from crawled files, and keep the
section and paragraph structure of the documents
and summaries. Additional data cleaning and processing details are in Appendix A.
We obtain 12, 228 GAO reports and 7, 238 CRS
reports of high quality evidenced by human inspection of 200 parsed reports. Collected GAO reports
and CRS reports have on average 6.9 and 4.6 sections, respectively. We split train, validation and
test set by publication date on each dataset, and
end up with 17519 training samples, 974 validation documents, and 973 test samples.
Notably, summaries of GAO reports are
written by experts, and are often structured
into three aspects in order: “Why GAO did
this study”—motivation and problem(s) under discussion, “What GAO found”—findings
of the report, and “What GAO recommends”—
suggestions and solutions to the problem(s). All but
three GAO summaries include “What GAO Found”.
The percentages of GAO summaries that contain
“Why GAO did this study” and “What GAO recommends” are 94.8% and 29.0%. For comparison, structured summaries are also observed on
PUBMED (Cohan et al., 2018) samples. Though
they do not contain explicit aspect labels, the summaries can often be broken down into “Introduction”, “Methods”, “Results”, and “Conclusion” via
keyword matching. Details about keyword choices
for each aspect are provided in Table 11 in Appendix D.
Comparison with Existing Long Document
Summarization Datasets. In Table 2, we compare GOVREPORT with several existing long document summarization datasets, including PUBMED
and ARXIV (Cohan et al., 2018) that consist of scientific publications; BILLSUM (Kornilova and Eidelman, 2019), a collection of congressional bills;
and BIGPATENT (Sharma et al., 2019), a corpus of
4www.gao.gov
5crsreports.congress.gov
Dataset # Doc Summary Doc Comp. Den.
# word # sent # word
PUBMED 133,215 202.4 6.8 3049.0 16.2 5.8
ARXIV 215,913 272.7 9.6 6029.9 39.8 3.8
BILLSUM 23,455 207.7 7.2 1813.0 13.6 4.1
BIGPATENT 1,341,362 116.5 3.7 3573.2 36.3 2.4
GOVREPORT 19,466 553.4 17.8 9409.4 19.0 7.3
Table 2: Statistics of GOVREPORT and existing long
document summarization datasets. Comp.: compression ratio, Den.: extractive fragment density (Grusky
et al., 2018). All values are mean over the whole
dataset except for the “# Doc” column. Documents and
summaries in GOVREPORT are significantly longer.
0 10 20 30 40 50 60 70 80 90 100
position in the source (%)
0
10
20
30
40
50
60
coverage of salient bigrams (%)
PubMed
arXiv
BillSum
BigPatent
GovReport
Figure 2: Percentage of unique salient bigrams accumulated from the start to X% of the source. Key information is spread over the documents in GOVREPORT,
highlighting the importance of understanding longer
text.
U.S. patent documents.
First, documents and summaries in GovReport
are significantly longer than prior datasets. Next,
we inspect the distribution of summary-worthy bigrams in the source by dividing each document
into ten equisized partitions. For each partition, we
count the occurrence of unique bigrams that also
appear in the reference, accumulated from the start
of the document to the end of the partition. Fig. 2
shows that key information is spread throughout
documents in GOVREPORT, with new salient bigrams being steadily added as more content is consumed. For ARXIV and BIGPATENT, only about
10% of new salient bigrams are accumulated in the
second half of the documents, reflecting the heavy
positional bias in these two datasets. In contrast, in
GovReport and BILLSUM, more than 18% of new
summary-worthy bigrams appear in the later half
of the articles, showing a more even distribution.
A similar trend is observed on unigrams. However,
BILLSUM has the shortest documents among the
five datasets.
5 Summary Evaluation with Cloze QA
This work aims to evaluate whether processing
more text improves both informativeness and faithfulness of abstractive summaries. In addition to
ROUGE (Lin, 2004) and human evaluation, we extend existing QA-based metric (Eyal et al., 2019)
and consider an entailment-based scorer.
QA-based Evaluation. We present a new faithfulness evaluation metric by extending the APES
score (Eyal et al., 2019). We follow APES to construct a set of cloze questions, {q}, from each reference summary by masking entities. Events, dates,
and numbers are also masked, as they are prevalent
in our data. Each masked phrase becomes the goldstandard answer aref for a question q. We do not
generate natural language questions (Durmus et al.,
2020; Wang et al., 2020a), due to the lack of accurate question generation models for the domains of
government reports and scientific papers.
QA models are trained by reading a question and
a context to label the answer span in the context.
We construct context by greedily selecting sentences that maximize the improvement of ROUGE2 recall when compared with the reference summary. If the answer aref cannot be found in the
context, the sample is excluded from training. We
train all QA models by fine-tuning BERT (Devlin
et al., 2019) to predict the answer span.
To evaluate the faithfulness of a system summary, APES uses the QA model to read the summary and a question q to label an answer asys. It
calculates a unigram F1 score by comparing asys
and aref . Different from APES, we further use the
QA model to read the context (sentences selected
from the source) and give an answer acxt to the
question q. We compute a unigram F1 by comparing asys and acxt, denoted as APESsrc. Given
that existing summarization models rarely rewrite
names or numbers correctly, our metric can better
capture faithfulness by using a gold-standard answer constructed from the source article than from
the human-written abstract.
To extract entities and events, we deploy a
state-of-the-art IE framework, OneIE (Lin et al.,
2020) on GOVREPORT. On PubMed, we retrain OneIE on Genia 2011 (BioNLP, 2011) and
2013 (BioNLP, 2013), and PubMed (Wei et al.,
2019) datasets to extract domain-specific entities
and events, such as entities of Gene and Disease.
We additionally include numbers and dates extracted by spaCy (Honnibal and Montani, 2017).
Entailment-based Evaluation. We further consider FactCC (Kryscinski et al., 2020), which evaluates factual consistency of a system summary by
predicting an entailment score between the source
and the summary. We reproduce their method on
our datasets.
Additional details for implementing the evaluation models and the entity extraction models are
given in Appendix B.
6 Experimental Results
In this section, we start with describing training
details in § 6.1. We then compare attention variants on documents of the same length (§ 6.2) and
study whether reading more text can generate more
informative summaries (§ 6.3). We further report
human evaluation on summary informativeness and
faithfulness as well as automatic faithfulness scores
(§ 6.4). Finally, we investigate whether automatic
metrics correlate with human judgment (§ 6.5).
6.1 Training Details
We fine-tune BART (Lewis et al., 2020) for all
experiments. We implement our models with PyTorch (Paszke et al., 2019) and Fairseq (Ott et al.,
2019). Additional position embeddings are initialized randomly for models that handle longer
inputs. The learning rate is set to 1 × 10−4
and
learning rate warm-up is applied for the first 10,000
steps. Adafactor (Shazeer and Stern, 2018) optimizer with a gradient clipping of 0.1 is used. All
models are trained on two Quadro RTX 6000 GPUs
with 24GB memory or one Quadro RTX 8000 with
48GB memory. We set a batch size of 2 per step
and accumulate gradient every 32 steps. During
test, we adopt a beam size of 4 and a length penalty
of 2 (Wu et al., 2016) on all datasets.
6.2 Comparing Attention Variants
Comparisons. We first experiment with articles
that are all truncated at 1024 tokens. For encoder
attentions, we consider the following variants: (1)
sliding WINDOW; (2) adaptive span (ADASPAN);
(3) GLOBAL tokens; (4) STRIDE; (5) RANDOM
tokens; (6) Linformer (LIN.); (7) locality sensitive
hashing (LSH); and (8) SINKHORN. We ensure
models are comparable by setting hyperparameters to satisfy w = ˆw = k = lbl = 2bs = 256,
so that models have similar memory complexity. For LSH attentions, we select l = 4 rounds
of hashing. Following prior work (Zaheer et al.,
GovReport (new) PubMed
System R-1 R-2 R-L R-1 R-2 R-L
FULL 52.83 20.50 50.14 45.36 18.74 40.26
Encoder variants w/ full enc-dec attn.
I. Fixed Patterns
WINDOW 50.78 18.59 48.10 42.74 16.83 37.96
+ GLOBAL 51.24 19.01 48.58 43.44 17.07 38.55
+ STRIDE 51.53 19.14 48.68 43.73 17.25 38.82
+ RANDOM 51.49 18.90 48.75 43.38 16.87 38.45
ADASPAN 50.76 18.69 48.13 43.42 17.16 38.60
+ GLOBAL 50.33 18.56 47.80 43.24 17.01 38.42
+ STRIDE 51.56 19.19 48.57 43.71 17.25 38.76
+ RANDOM 51.39 18.89 48.74 43.28 16.87 38.45
II. Low-Rank Methods
LIN. 50.70 18.48 47.85 43.65 17.12 38.71
III. Learnable Patterns
LSH 51.95 19.36 48.85 44.74 18.07 39.76
SINKHORN 53.00∗
20.05∗
50.25∗
45.10 18.40∗
40.11∗
Enc-dec variants w/ full encoder attn.
LIN. 47.79 14.93 45.15 45.16 17.66 40.25
HEPOS (ours) 51.05∗
19.44∗
48.51∗
45.80∗
18.61∗
40.69∗
Enc-dec variants w/ Sinkhorn encoder attn.
LIN. 42.90 12.86 40.32 44.84 17.65 39.98
HEPOS (ours) 51.34∗
19.09∗
48.73∗
44.85 18.19∗
39.91
Table 3: Results on evaluating encoder and encoderdecoder attentions on input of the same length. Best
ROUGE scores of fixed patterns, learnable patterns,
and enc-dec attentions are in red, orange, and purple,
respectively. ∗: significantly better than comparison(s)
using the same encoder or enc-dec attention (approximation randomization test, p < 0.0005).
2020), we combine GLOBAL, STRIDE, and RANDOM with WINDOW and ADASPAN, where we set
g = n
2/s = r = 128 for a fair comparison. We
adapt Linformer to encoder-decoder attentions to
compare with HEPOS, where we use sh = n/k = 4
for all experiments. Finally, we report results using FULL, i.e., the original, encoder and encoderdecoder attentions.
Results. Among all encoder variants, learnable
patterns perform the best, approaching the performance of full attentions on both GovReport and
PubMed, as shown in Table 3. Within learnable patterns, Sinkhorn attention consistently obtains better
ROUGE scores. Moreover, combining techniques
in fixed patterns is more effective than simply using window-based sparse attentions, though with
an increased memory cost.
For encoder-decoder attentions, HEPOS consistently yields higher ROUGE scores than Linformer
on both datasets, using either full or Sinkhorn encoder. Notably, coupled with a Sinkhorn attention,
our model’s performance matches the variant using
GovReport PubMed
System (MAXLEN) R-1 R-2 R-L R-1 R-2 R-L
Baselines
PEGASUS (1024) – – – 45.97 20.15 41.34
TLM (full) – – – 42.13 16.27 39.21
SEAL (full) – – – 46.50 20.10 42.20
DANCER (full) – – – 46.34 19.97 42.42
BIGBIRD (3072) – – – 46.32 20.65 42.33
Encoder variants w/ full enc-dec attn.
FULL (1024) 52.83 20.50 50.14 45.36 18.74 40.26
STRIDE (4096) 54.29 20.80 51.35 46.95 19.98 41.67
LIN. (3072) 44.84 13.87 41.94 43.69 16.35 38.66
LSH (4096) 54.75 21.36 51.27 47.54 20.79 42.22
SINKHORN (5120) 55.45 21.45 52.48 47.96 20.78 42.53
Encoder variants w/ HEPOS enc-dec attn. (ours)
LSH (7168) 55.00 21.13 51.67 48.12 21.06 42.72
SINKHORN (10240) 56.86 22.62 53.82 47.93 20.74 42.58
Table 4: ROUGE scores for models trained on the same
GPU. SINKHORN with HEPOS enc-dec attention and
LSH with HEPOS both read more text and obtain significantly better scores than other models on GovReport and PubMed (p < 0.0005).
System (MAXLEN) R-1 R-2 R-L
Baselines
PEGASUS (1024) 44.21 16.95 38.83
TLM (full) 41.62 14.69 38.03
SEAL (full) 44.3 18.0 39.3
DANCER (full) 45.01 17.60 40.56
BIGBIRD (3072) 46.63 19.02 41.77
Encoder variants w/ HEPOS enc-dec attn. (ours)
LSH (7168) 48.24 20.26 41.78
SINKHORN (10240) 47.87 20.00 41.50
Table 5: Automatic evaluation on arXiv. Our best
model yields better ROUGE scores than previous stateof-the-art models.
full encoder attention, implying the effectiveness
of HEPOS on both identifying the salient content
and capturing the global context.
6.3 Reading More Input Boosts
Informativeness
We investigate whether processing more words generates more informative summaries.
Comparisons include recent top-performing abstractive models: PEGASUS (Zhang et al., 2019),
a large pre-trained summarization model with
truncated inputs; TLM (Pilault et al., 2020),
DANCER (Gidiotis and Tsoumakas, 2020), and
SEAL (Zhao et al., 2020), all of them using hybrid
extract-then-abstract methods; and BIGBIRD (Zaheer et al., 2020), which combines sliding window,
global and random token attentions in the encoder.
For encoder variants, we pick the best performing model from fixed patterns to be combined with
full encoder-decoder attention, i.e., sliding window
with stride (STRIDE), low-rank method (LIN.), and
learnable patterns (LSH and SINKHORM). We then
combine learnable patterns with HEPOS to support
processing more text. All models consume as long
an input as the memory allows.
Results. Overall, models that read more text obtain
higher ROUGE scores, according to results on GovReport and PubMed in Table 4. First, different encoder variants with full encoder-decoder attentions
attain better results than the full attentions baseline
except Linformer. Second, adding HEPOS encoderdecoder attention almost doubles the words that
can be processed and further improves the performance. This highlights the importance of handling
both encoder attentions and encoder-decoder attentions efficiently. Notably, HEPOS with an LSH
encoder achieves new state-of-the-art results on
PubMed, outperforming BigBird which only uses
sparse attentions on the encoder. We also report
performances of our two best models with HEPOS
on arXiv in Table 5, and they outperform all competitive abstractive models.
As can be seen from the sample summaries in
Fig. 3, our model that reads in 10k tokens generates
more informative summary than the full attention
model that only processes 1k tokens. Fig. 4 further
shows that ROUGE-2 scores can be consistently
lifted when reading more input, with similar trends
observed on ROUGE-1 and ROUGE-L. More sample outputs are presented in Appendix C.
6.4 Reading More Input Improves
Faithfulness
Here we first show human evaluation results on
informativeness and unfaithful errors in the generated summaries. We sample 100 documents from
GovReport and PubMed (50 each) with structured
references that are labeled with aspects as described
in § 4 and Appendix D. Each sample is evaluated
by two fluent English speakers, who have cumulatively annotated tens of thousands of sentences
for the same tasks before this work. Annotators
are asked to label each summary sentence with
an aspect and then decide whether it contains any
type of error. Three types of unfaithful errors are
considered: (i) hallucination—fabricating content
not present in the input, (ii) deletion—incorrectly
Human-written Summary:
In fiscal year 2018, Medicaid covered approximately 75
million individuals at an estimated cost of $629 billion,
$393 billion of which were federal funds. (...)
While CMS is generally required to disallow, or recoup,
federal funds from states for eligibility-related improper
payments if the state’s eligibility error rate exceeds 3 percent, it has not done so for decades, because the method it
used for calculating eligibility error rates was found to
be insufficient for that purpose. To address this, in July
2017, CMS issued revised procedures through which it
can recoup funds for eligibility errors, beginning in fiscal
year 2022. (...)
Model w/ full attn.:
Medicaid is a federal-state program that provides health
care coverage to low-income individuals and families. (...)
CMS officials stated that they have provided states with
guidance on how to use data from SSA’s automated system
for eligibility determinations, (...)
CMS officials said that they did not have guidance on
when states should use SSA data to evaluate eligibility based
on nonfinancial or financial criteria. (...)
Model w/ HEPOS enc-dec attn. (ours):
The Patient Protection and Affordable Care Act (PPACA)
expanded Medicaid coverage to millions of low-income
adults and children with disabilities and their eligible dependents. (...)
The selected states also reported that they did not have adequate processes to address these issues. CMS has taken
steps to improve its oversight of the Medicaid program,
including issuing guidance to states on the use of MAGIexempt bases for determining eligibility, but these efforts
have not been fully implemented. (...)
Figure 3: Sample summaries for a government report.
The model with truncated input generates unfaithful
content. HEPOS attention with a Sinkhorn encoder
covers more salient information.
2k 4k 6k 8k 10k
Length
18
19
20
21
22
ROUGE-2
PubMed
GovReport
Figure 4: Summarizing articles truncated at different
lengths by the best models: LSH (7168)+HEPOS on
PubMed and SINKHORN (10240)+HEPOS on GovReport. Reading more consistently improves ROUGE-2.
deleting crucial entities, events, or clauses, and (iii)
false concatenation—inappropriately concatenating components from different sentences. 1 is given
if any judge determines that a certain type of error
exists in the sentence, 0 otherwise.
After reading the full summaries, each judge also
scores aspect-level informativeness—whether the
System (MaxLen) Inf.↑ Hal.↓ Del.↓ Concat.↓
GovReport
Encoder variants w/ full enc-dec attn.
FULL (1024) 3.29 15.2% 3.5% 9.5%
SINKHORN (5120) 3.32 11.0% 2.3% 9.4%
Encoder variant w/ HEPOS enc-dec attn. (ours)
SINKHORN (10240) 3.53 11.5% 3.4% 8.8%
PubMed
Encoder variants w/ full enc-dec attn.
FULL (1024) 3.27 20.1% 2.8% 14.3%
SINKHORN (5120) 3.94 4.8% 1.6% 9.6%
Encoder variant w/ HEPOS enc-dec attn. (ours)
SINKHORN (10240) 4.18 3.5% 2.2% 9.1%
Table 6: Human evaluation on informativeness (Inf.)
(1-to-5), and percentages of unfaithful errors due to
hallucination (Hal.), deletion (Del.), and false concatenation (Concat.). Inter-rater agreement with Krippendorf’s α for all columns: 0.59, 0.59, 0.53 and 0.60.
summary covers important information of an aspect
when compared with the reference. All system summaries and references are presented in a random
order. Human evaluation guidelines and sample
summaries for different aspects are included in Appendix D.
Results. Overall, reading more text significantly
improves informativeness as well as reduces fabricated content. From Table 6, we observe that
HEPOS attention, combined with a SINKHORN encoder, obtains better informativeness scores than
comparisons that read in less text on both datasets.
This echos results from automatic evaluation in
the previous section. Moreover, both models that
use efficient attentions reduce unfaithfulness, especially hallucination errors, when compared with
the full attention model, which only reads 1024 tokens. As the models read more content, they learn
to surface more factual and richer content in the
summaries, as seen in Fig. 3.
Next, we explore if reading more helps correctly
reflect the content in documents’ later sections. We
plot aspect-level human ratings of informativeness
and unfaithful errors on PubMed and GovReport
in Fig. 5 and Fig. 6. We report percentages of sentences with unfaithful errors by majority voting
(i.e., at least one error is found by both annotators in the sentence). As can be seen, our models
consistently improve informativeness and reduce
errors across sections, especially for “Results” and
“Conclusions” on PubMed and “What GAO recommends” on GovReport—these sections often
appear in the later part of the source documents.
Introduction Methods Results Conclusion
Informativeness
0
1
2
3
4
Score
4.27
2.88 2.75
2.24
4.48
4.02
3.67 3.59
4.54
4.20
3.79
4.21
Introduction Methods Results Conclusion
Unfaithful Errors
0
10
20
30
40
50
60
Precentage (%)
7.2
15.5
62.0
53.8
8.5 12.3
31.0
15.7
7.4 7.1
25.0
10.1
Full(1k)+Full Sinkhorn(5k)+Full Sinkhorn(10k)+Hepos
Figure 5: Aspect-level informativeness and percentages of sentences containing unfaithful errors as labeled by both human judges on PubMed. Models with
efficient attentions reduce errors for later sections in the
sources, e.g., “Results" and “Conclusion".
Why GAO did this study What GAO found What GAO recommends
Informativeness
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
Score
3.75
3.49
2.64
3.80
3.55
2.61
4.01
3.60
2.99
Why GAO did this study What GAO found What GAO recommends
Unfaithful Errors
0
2
4
6
8
10
12
14
Precentage (%)
12.4
14.0
6.7
10.9 10.7
3.7
5.3
10.6
2.8
Full(1k)+Full Sinkhorn(5k)+Full Sinkhorn(10k)+Hepos
Figure 6: Aspect-level informativeness and percentages of sentences with unfaithful errors on GovReport.
Especially, we find that the full attention model
tends to produce fabricated numbers in resultant
summaries, whereas our models are able to correct
them.
Lastly, we report the entailment-based FactCC
and QA scores APES and APESsrc for top performing models in Table 7. The results again show that
consuming longer input leads to more faithful summaries, though the differences are less pronounced.
6.5 Correlations between Human and
Automatic Metrics
Finally, we study whether the faithfulness evaluation metrics correlate with human judgment. As
shown in Table 8, on both government reports
and scientific papers, QA metrics are better correlated with human ratings, with our newly pro-
GovReport PubMed
System (MaxLen) F. APES APESsrc F. APES APESsrc
FULL (1024) 58.9 42.7 42.7 74.6 43.2 31.5
Encoder variants w/ full enc-dec attn.
STRIDE (4096) 55.3 43.1 42.5 72.7 43.8 31.9
LIN. (3072) 48.4 35.7 36.3 67.7 39.3 29.5
LSH (4096) 55.7 44.0 43.6 73.2 46.7 35.1
SINKHORN (5120) 57.0 43.6 42.1 72.9 46.8 35.4
Encoder variants w/ HEPOS enc-dec attn. (ours)
LSH (7168) 59.6 44.0 44.2 73.3 47.5 35.6
SINKHORN (10240) 60.1 44.0 44.3 71.9 46.2 34.8
Table 7: Evaluation with FactCC (F.), APES, and the
new APESsrc metric, with higher numbers indicating
more faithful summaries.
GovReport PubMed
Metric Inf.↑ Err.↓ Inf.↑ Err.↓
FactCC 0.07 -0.08 0.10 -0.14
APES 0.16 -0.15 0.25 -0.31
APESsrc 0.21 -0.23∗ 0.32∗ -0.32
Table 8: Pearson correlation between human ratings
and metrics. We use aggregated unfaithful errors (Err.).
∗: significantly better than other metrics based on
William’s test (Williams, 1959) (p < 0.05).
posed APESsrc being the stronger of the two. After inspection, we find that human-written summaries contain paraphrases or acronyms that APES
cannot capture via strict lexical matching. For instance, for the question “Diabetes may worsen
in patients”, the reference answer is “death rate”,
whereas answers from the source and the system
summary are both “mortality”. APESsrc captures
this, but not APES.
7 Additional Related Work
Summarizing long inputs has been investigated in
many domains, including books (Mihalcea and
Ceylan, 2007), patents (Trappey et al., 2009),
movie scripts (Gorinski and Lapata, 2015), and scientific publications (Qazvinian and Radev, 2008).
However, the datasets are often too small to train
neural models. Cohan et al. (2018) publish two
large-scale datasets by collecting articles from
ARXIV and PUBMED. Popular methods rely on
extractive summarizers that identify salient sentences based on positional information (Dong et al.,
2020) or combined global and local contexts (Xiao
and Carenini, 2019), where each sentence is represented as aggregated word embeddings. However,
extractive summaries are often redundant and incoherent, highlighting the need for handling long
documents via abstractive summarization.
To that end, extract-then-abstract methods are
proposed. For example, Pilault et al. (2020) first
extract relevant sentences and then rewrite them
into paper abstracts. Our work is in line with building end-to-end abstractive summarization models
for long input. Cohan et al. (2018) design a hierarchical encoder to read different sections separately,
and then use combined attentions over words and
sections to generate the summary. Multiple agents
are created to read segments separately, and then
collaboratively write an abstract (Celikyilmaz et al.,
2018). However, both work truncates articles to
2K words. Although efficient encoder attentions
have been studied in Zaheer et al. (2020) for abstractive summarization, at most 3K tokens can be
consumed by their models. Our HEPOS encoderdecoder attention are able to process more than
10K tokens, significantly improving summary informativeness and faithfulness.
8 Conclusion
We investigate efficient attentions for long document summarization. We propose a novel encoderdecoder attention, HEPOS, based on head-wise positional strides that can effectively identify salient
content. Models based on HEPOS attention can process at least twice as many words and produce more
informative summaries with less unfaithful errors,
according to both automatic evaluation and human
evaluation. We further show that our new cloze QA
metric better correlates with human judgment than
prior faithfulness evaluation metrics.
